{"status":"success","data":{"groups":[{"name":"node-basics","file":"/etc/prometheus/alert_rules.yml","rules":[{"state":"inactive","name":"NodeExporterDown","query":"up{job=\"node\"} == 0","duration":120,"keepFiringFor":0,"labels":{"severity":"page"},"annotations":{"description":"Prometheus cannot scrape node_exporter for 2m. Investigate node reachability, service, port, firewall.","summary":"Node exporter down on {{ $labels.instance }}"},"alerts":[],"health":"ok","evaluationTime":0.000876773,"lastEvaluation":"2026-02-09T21:21:48.591980376Z","type":"alerting"},{"state":"inactive","name":"DiskWillFillSoon","query":"(node_filesystem_avail_bytes{fstype!~\"tmpfs|overlay\"} / node_filesystem_size_bytes{fstype!~\"tmpfs|overlay\"}) \u003c 0.1","duration":600,"keepFiringFor":0,"labels":{"severity":"page"},"annotations":{"description":"Less than 10% disk free for 10m. Investigate largest directories/files, cleanup, or expand disk.","summary":"Disk space critical on {{ $labels.instance }} ({{ $labels.mountpoint }})"},"alerts":[],"health":"ok","evaluationTime":0.000651821,"lastEvaluation":"2026-02-09T21:21:48.592885888Z","type":"alerting"},{"state":"inactive","name":"HighCpuUsage","query":"100 - (avg by (instance) (rate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100) \u003e 85","duration":900,"keepFiringFor":0,"labels":{"severity":"ticket"},"annotations":{"description":"CPU >85% for 15m. Check top processes, load, and whether there is user impact.","summary":"High CPU on {{ $labels.instance }}"},"alerts":[],"health":"ok","evaluationTime":0.000709659,"lastEvaluation":"2026-02-09T21:21:48.593549676Z","type":"alerting"},{"state":"inactive","name":"NodeRebooted","query":"changes(node_boot_time_seconds[15m]) \u003e 0","duration":0,"keepFiringFor":0,"labels":{"severity":"info"},"annotations":{"description":"node_boot_time_seconds changed within 15m. Confirm change record and expected reboot.","summary":"Node reboot detected on {{ $labels.instance }}"},"alerts":[],"health":"ok","evaluationTime":0.000204583,"lastEvaluation":"2026-02-09T21:21:48.594276863Z","type":"alerting"}],"interval":15,"limit":0,"evaluationTime":0.00253623,"lastEvaluation":"2026-02-09T21:21:48.591950005Z"},{"name":"slo-burnrate","file":"/etc/prometheus/slo_alert_rules.yml","rules":[{"state":"inactive","name":"SLOBurnRateFast","query":"(1 - avg_over_time(up{job=\"node\"}[5m])) \u003e 0.02","duration":300,"keepFiringFor":0,"labels":{"severity":"page"},"annotations":{"description":"Availability is dropping sharply over 5m windows. Risk of missing SLO if sustained. Investigate immediately.","summary":"Fast error budget burn (node monitoring availability)"},"alerts":[],"health":"ok","evaluationTime":0.000662252,"lastEvaluation":"2026-02-09T21:21:37.152477959Z","type":"alerting"},{"state":"inactive","name":"SLOBurnRateSlow","query":"(1 - avg_over_time(up{job=\"node\"}[30m])) \u003e 0.005","duration":1800,"keepFiringFor":0,"labels":{"severity":"ticket"},"annotations":{"description":"Availability is degrading over longer windows. Investigate in business hours and reduce error budget consumption.","summary":"Slow error budget burn (node monitoring availability)"},"alerts":[],"health":"ok","evaluationTime":0.00018066,"lastEvaluation":"2026-02-09T21:21:37.153155805Z","type":"alerting"}],"interval":15,"limit":0,"evaluationTime":0.000922924,"lastEvaluation":"2026-02-09T21:21:37.152418464Z"}]}}